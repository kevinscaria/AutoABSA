{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1669923415138,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"h7ZSuF2jMRhu"},"outputs":[],"source":["try:\n","    import google.colab\n","    IN_COLAB = True\n","except:\n","    IN_COLAB = False"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":179107,"status":"ok","timestamp":1669923626068,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"E1rMgFtcMJUQ","outputId":"ccf05c14-1364-4771-a255-c9bb7e046235"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 3.3 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 36.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 54.1 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.24.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n","\u001b[K     |████████████████████████████████| 451 kB 4.0 MB/s \n","\u001b[?25hRequirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 56.1 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting xxhash\n","  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 49.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 57.6 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.7.1 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting evaluate\n","  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n","\u001b[K     |████████████████████████████████| 72 kB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.11.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.3.6)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.7.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.21.6)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2022.11.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.3.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from evaluate) (4.64.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from evaluate) (21.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.70.14)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from evaluate) (3.1.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.23.0)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.1.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->evaluate) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (3.0.4)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2022.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n","Installing collected packages: evaluate\n","Successfully installed evaluate-0.3.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 3.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from vaderSentiment) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (1.25.11)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 4.2 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","2022-12-01 19:38:35.646093: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-lg==3.4.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n","\u001b[K     |████████████████████████████████| 587.7 MB 16 kB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-lg==3.4.1) (3.4.3)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.23.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.9.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (5.2.1)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.1.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-3.4.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_lg')\n"]}],"source":["if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    !pip install transformers\n","    !pip install datasets\n","    !pip install evaluate\n","    !pip install vaderSentiment\n","    !pip install sentencepiece\n","    !python -m spacy download en_core_web_lg"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6879,"status":"ok","timestamp":1669923663650,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"45-ZdKZ-7OMx"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import re, os\n","import pandas as pd\n","import numpy as np\n","from collections import defaultdict\n","import xml.etree.ElementTree as ET\n","\n","from sklearn.metrics.pairwise import rbf_kernel\n","\n","import torch\n","from transformers import AutoTokenizer, BertTokenizerFast, BertModel, BertConfig\n","\n","# import nltk\n","# from nltk import Tree\n","\n","# import stanza\n","# from textblob import TextBlob\n","# from allennlp.predictors.predictor import Predictor\n","\n","import spacy\n","from spacy import displacy\n","from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER, CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n","from spacy.util import compile_infix_regex\n","nlp = spacy.load('en_core_web_lg')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2310,"status":"ok","timestamp":1669923671213,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"AJYn0jlt7OMz"},"outputs":[],"source":["# Set spacy\n","infixes = (\n","        LIST_ELLIPSES\n","        + LIST_ICONS\n","        + [\n","            r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n","            r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n","                al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n","            ),\n","            r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n","            r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n","        ]\n","    )\n","    \n","infix_re = compile_infix_regex(infixes)\n","nlp = spacy.load('en_core_web_lg')\n","nlp.tokenizer.infix_finditer = infix_re.finditer"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669923855579,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"_zM0QGYn7OMz"},"outputs":[],"source":["# Set directory\n","if IN_COLAB:\n","    root_path = '/content/drive/Othercomputers/DT Laptop/Sentiment Analysis/UnsupervisedABSA'\n","else:\n","    root_path = os.getcwd()\n","\n","# Set root path\n","os.chdir(root_path)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":768,"status":"ok","timestamp":1669923815739,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"F3KZA1iF7OMz"},"outputs":[],"source":["def parse_xml_data(root):\n","    \"\"\"Parses xml data to extract aspect terms and aspect categories from each sentence\"\"\"\n","    \n","    reviews = []\n","\n","    for sentence in root.findall(\"sentence\"):\n","        entry = {}\n","        aspect_terms = []\n","        aspect_categories = []\n","        \n","        if sentence.find(\"aspectTerms\"):\n","            for aspect_term in sentence.find(\"aspectTerms\").findall(\"aspectTerm\"):\n","                aspect_terms.append((aspect_term.get(\"term\"), aspect_term.get(\"polarity\")))\n","                \n","        if sentence.find(\"aspectCategories\"):\n","            for aspect_category in sentence.find(\"aspectCategories\").findall(\"aspectCategory\"):\n","                aspect_categories.append((aspect_category.get(\"category\"), aspect_category.get(\"polarity\")))\n","                \n","        entry[\"text\"] = sentence[0].text\n","        entry[\"terms\"] = aspect_terms\n","        entry[\"aspects\"] = aspect_categories\n","        \n","        reviews.append(entry)\n","\n","    reviews_df = pd.DataFrame(reviews)\n","    \n","    return reviews_df"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":13816,"status":"ok","timestamp":1669923876285,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"bfh0ZsdV7OM0","outputId":"da328afa-1871-46d4-8213-c88746391451"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at ./AutoABSA/rest_pft were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at ./AutoABSA/rest_pft and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# # Initialize pretrained models for subtasks\n","# coref_res_url = \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\"\n","# coreference_resolver = Predictor.from_path(coref_res_url)\n","\n","# stanzanlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos,lemma,depparse')\n","\n","tokenizer = BertTokenizerFast.from_pretrained(\"./AutoABSA/rest_pft\")\n","model = BertModel.from_pretrained(\"./AutoABSA/rest_pft\", output_hidden_states=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":1071,"status":"ok","timestamp":1669923877345,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"9M0Dqcqe7OM0"},"outputs":[],"source":["# Parsing XML tree\n","tree = ET.parse('./semeval14/ABSA_TrainData/Restaurants_Train.xml')\n","root = tree.getroot()\n","df = parse_xml_data(root)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1669923877345,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"36oTRkY-7OM0","outputId":"910c4178-346e-4fb0-a9d2-7e0db58ecbd2"},"outputs":[{"data":{"text/plain":["(3044, 3)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["df.shape"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669923877663,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"Ynb96w4i7OM1"},"outputs":[],"source":["# def resolve_coreference(text, remove_words = ['the']):\n","#     coref_res_pred = coreference_resolver.predict(text)\n","#     coref_resolved_text = coreference_resolver.coref_resolved(text)\n","#     for cluster in coref_res_pred['clusters']:\n","#         coref_word, coref_dep = cluster[0], cluster[1]\n","#         coref_list = text.split()[coref_word[0]:coref_word[1]+1]\n","#         for bad_word in remove_words:\n","#             try:\n","#                 coref_list.remove(bad_word)\n","#             except:\n","#                 pass\n","#         coref_hyphenated_text = '-'.join(coref_list)\n","#         coref_hyphenated_text = re.sub('[^A-Za-z0-9-]+', ' ', coref_hyphenated_text).strip()\n","#         coref_spaced_text = ' '.join(coref_list)\n","#         coref_spaced_text = re.sub('[^A-Za-z0-9-]+', ' ', coref_spaced_text).strip()\n","#         coref_resolved_text = coref_resolved_text.replace(coref_spaced_text, coref_hyphenated_text)\n","#     return coref_resolved_text"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1669923880219,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"xD8_m_xm7OM1"},"outputs":[],"source":["def extract_candidate_terms(text):\n","    \"\"\"Extracts all nouns from text as aspect term candidates\"\"\"\n","    \n","    req_tag = ['NN', 'NNPS']\n","    candidate_terms = []\n","    \n","    for token in nlp(text):\n","        # print(token, token.tag_)\n","        if (\n","            token.tag_ in req_tag and # token must be noun\n","            token.shape_ != 'x' and # token mustn't be only one character\n","            token.shape_ != 'xx' and # token mustn't be only two character\n","            token.shape_ != 'xxx' # token mustn't be only three character\n","        ):\n","            candidate_terms.append(token.lemma_)\n","            \n","    return candidate_terms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8fZvRO07OM1"},"outputs":[],"source":["# # Resolve Coreferences\n","# df['text'] = df['text'].apply(lambda x: resolve_coreference(x) if len(x.split())>4 else x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4k3Y5ZE7OM1"},"outputs":[],"source":["# # Extract aspect term candidates\n","# df['aspect_term_candidates'] = df.text.apply(extract_candidate_terms)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":302,"status":"ok","timestamp":1669923882863,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"XsXwKWV27OM1","outputId":"bbe5111d-e685-442f-de54-6cc540852d0b"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-dda629a1-38e2-447d-8baf-de3a3ead4492\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>terms</th>\n","      <th>aspects</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>But the staff was so horrible to us.</td>\n","      <td>[(staff, negative)]</td>\n","      <td>[(service, negative)]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>To be completely fair, the only redeeming fact...</td>\n","      <td>[(food, positive)]</td>\n","      <td>[(food, positive), (anecdotes/miscellaneous, n...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The food is uniformly exceptional, with a very...</td>\n","      <td>[(food, positive), (kitchen, positive), (menu,...</td>\n","      <td>[(food, positive)]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Where Gabriela personaly greets you and recomm...</td>\n","      <td>[]</td>\n","      <td>[(service, positive)]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>For those that go once and don't enjoy it, all...</td>\n","      <td>[]</td>\n","      <td>[(anecdotes/miscellaneous, positive)]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Not only was the food outstanding, but the lit...</td>\n","      <td>[(food, positive), (perks, positive)]</td>\n","      <td>[(food, positive), (service, positive)]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>It is very overpriced and not very tasty.</td>\n","      <td>[]</td>\n","      <td>[(food, negative), (price, negative)]</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Our agreed favorite is the orrechiete with sau...</td>\n","      <td>[(orrechiete with sausage and chicken, positiv...</td>\n","      <td>[(food, positive), (service, positive)]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>The Bagels have an outstanding taste with a te...</td>\n","      <td>[(Bagels, positive)]</td>\n","      <td>[(food, positive)]</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Nevertheless the food itself is pretty good.</td>\n","      <td>[(food, positive)]</td>\n","      <td>[(food, positive)]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dda629a1-38e2-447d-8baf-de3a3ead4492')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-dda629a1-38e2-447d-8baf-de3a3ead4492 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-dda629a1-38e2-447d-8baf-de3a3ead4492');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                text  \\\n","0               But the staff was so horrible to us.   \n","1  To be completely fair, the only redeeming fact...   \n","2  The food is uniformly exceptional, with a very...   \n","3  Where Gabriela personaly greets you and recomm...   \n","4  For those that go once and don't enjoy it, all...   \n","5  Not only was the food outstanding, but the lit...   \n","6          It is very overpriced and not very tasty.   \n","7  Our agreed favorite is the orrechiete with sau...   \n","8  The Bagels have an outstanding taste with a te...   \n","9       Nevertheless the food itself is pretty good.   \n","\n","                                               terms  \\\n","0                                [(staff, negative)]   \n","1                                 [(food, positive)]   \n","2  [(food, positive), (kitchen, positive), (menu,...   \n","3                                                 []   \n","4                                                 []   \n","5              [(food, positive), (perks, positive)]   \n","6                                                 []   \n","7  [(orrechiete with sausage and chicken, positiv...   \n","8                               [(Bagels, positive)]   \n","9                                 [(food, positive)]   \n","\n","                                             aspects  \n","0                              [(service, negative)]  \n","1  [(food, positive), (anecdotes/miscellaneous, n...  \n","2                                 [(food, positive)]  \n","3                              [(service, positive)]  \n","4              [(anecdotes/miscellaneous, positive)]  \n","5            [(food, positive), (service, positive)]  \n","6              [(food, negative), (price, negative)]  \n","7            [(food, positive), (service, positive)]  \n","8                                 [(food, positive)]  \n","9                                 [(food, positive)]  "]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["df.head(10)"]},{"cell_type":"markdown","metadata":{"id":"WBr-0cG77OM1"},"source":["## Domain Specific Embedding Approach - Opinion Word Mining"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":306,"status":"ok","timestamp":1669923952730,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"qP7f3fyN7OM2"},"outputs":[],"source":["class OpinionWordMiner:\n","    def __init__(self, tokenizer, gamma = 0.0005, we_layer_list=[-1]):\n","        self.tokenizer = tokenizer\n","        self.gamma = gamma\n","        \n","        \n","    def align_subwords_embeddings(self, marked_text, orig_word_embeddings):\n","        \"\"\"\n","        This function merges the word embeddings of subwords from BERT tokenizer and averages them.\n","        \"\"\"\n","        subword_token_ids = self.tokenizer(marked_text).word_ids()[1:-1]\n","        # print(subword_token_ids)\n","        we = np.zeros((len(set(subword_token_ids)), orig_word_embeddings.shape[1]))\n","        for i, idx in enumerate(subword_token_ids):\n","            # print(i, idx)\n","            if np.any(we[idx, :]):\n","                we[idx, :] = np.mean(np.vstack((we[idx, :], orig_word_embeddings[i, :])), axis=0)\n","            else:\n","                we[idx, :] = orig_word_embeddings[i, :]\n","        return we\n","\n","    def get_word_embeddings(self, marked_text):\n","        \"\"\"\n","        This function extracts the word/token embeddings from the specified hidden layers.\"\"\"\n","        tokenized_text = tokenizer.tokenize(marked_text)\n","        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","        segments_ids = [1] * len(tokenized_text)\n","        tokens_tensor = torch.tensor([indexed_tokens])\n","        segments_tensors = torch.tensor([segments_ids])\n","        with torch.no_grad():\n","            outputs = model(tokens_tensor, segments_tensors)\n","            hidden_states = outputs[2]\n","            \n","        token_embeddings = torch.stack(hidden_states, dim=0)\n","        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n","        token_embeddings = token_embeddings.permute(1,0,2)\n","\n","        token_vecs_cat = []\n","        for token in token_embeddings:\n","            cat_vec = torch.cat((\n","                token[0],\n","                token[-1]\n","            ), dim=0)\n","            token_vecs_cat.append(cat_vec.detach().numpy())\n","        return np.array(token_vecs_cat), tokenized_text\n","    \n","    def mine_opinion_words(self, text, aspect_word):\n","        idx2word_map = defaultdict(list)\n","        we_tokens_pos_tags, we_tokens_deps = [], []\n","        \n","        marked_text = \"[CLS] \" + text + \" [SEP]\"\n","        wts, _ = self.get_word_embeddings(marked_text)\n","        # print(wts.shape)\n","        word_emb = self.align_subwords_embeddings(marked_text, wts)\n","\n","        for token, subword_idx in zip(_, self.tokenizer(marked_text).word_ids()[1:-1]):\n","            idx2word_map[subword_idx].append(token.replace('#', ''))\n","\n","        for key, val in idx2word_map.items():\n","            idx2word_map[key] = ''.join(val)\n","\n","        word2idx_map = {val:key for key, val in idx2word_map.items()}\n","        # print(word2idx_map)\n","        spacy_tokens_pos_tags = {token.text.lower():token.pos_ for token in nlp(text)}\n","        spacy_tokens_deps = {token.text.lower():token.dep_ for token in nlp(text)}\n","\n","        for we_tokens in list(idx2word_map.values()):\n","            if we_tokens in spacy_tokens_pos_tags:\n","                we_tokens_pos_tags.append(spacy_tokens_pos_tags[we_tokens])\n","                we_tokens_deps.append(spacy_tokens_deps[we_tokens])\n","            else:\n","                we_tokens_pos_tags.append(None)\n","                we_tokens_deps.append(None)\n","\n","        word_i = word2idx_map[aspect_word]\n","        # print('WI: ', word_i)\n","        dep_df = pd.DataFrame(rbf_kernel(word_emb, word_emb, self.gamma)[word_i], columns = ['dep'], \n","                              index = idx2word_map.values())\n","        dep_df['pos'] = we_tokens_pos_tags\n","        dep_df['deps'] = we_tokens_deps\n","        dep_df = dep_df.reset_index().rename(columns = {'index':'opwo'})\n","        display(dep_df)\n","        asp_word_dep = dep_df[(dep_df['opwo'] == aspect_word)]['deps'].values[0]\n","        # print('ASP DEP: ', asp_word_dep)\n","        if asp_word_dep in ['dobj', 'pobj', 'conj']:\n","            display(dep_df[\n","#                 (dep_df['pos'] == 'ADJ')|\\\n","                                  (dep_df['deps'].isin(['amod', 'dobj']))].sort_values(by = 'dep', \n","                                                                          ascending = False))\n","            opinion_word = dep_df[\n","#                 (dep_df['pos'] == 'ADJ')|\\\n","                                  (dep_df['deps'].isin(['amod', 'dobj']))].sort_values(by = 'dep', \n","                                                                          ascending = False).head(1).index.values[0]\n","        else:\n","#             nsubj:acomp\n","            display(dep_df[\n","#                 (dep_df['pos'] == 'ADJ')|\\\n","                                  (dep_df['deps'].isin(['acomp']))].sort_values(by = 'dep', \n","                                                                          ascending = False))\n","            opinion_word = dep_df[\n","#                 (dep_df['pos'] == 'ADJ')|\\\n","                                  (dep_df['deps'].isin(['acomp']))].sort_values(by = 'dep', \n","                                                                          ascending = False).head(1).index.values[0]\n","            \n","        #left check\n","        l1text, l2text = '', ''\n","        try:\n","            if dep_df.iloc[opinion_word-1]['pos'] == 'PART':\n","                l1text = dep_df.iloc[opinion_word-1]['opwo'] + ' '\n","        except:\n","            pass\n","        try:\n","            if dep_df.iloc[opinion_word1-2]['pos'] == 'PART':\n","                l2text = dep_df.iloc[opinion_word-2]['opwo'] + ' '\n","        except:\n","            pass\n","        \n","        # Right check\n","        r1text, r2text = '', ''\n","        try:\n","            if dep_df.iloc[opinion_word+1]['pos'] == 'PART':\n","                r1text = ' ' + dep_df.iloc[opinion_word+1]['opwo']\n","        except:\n","            pass\n","        try:\n","            if dep_df.iloc[opinion_word1+2]['pos'] == 'PART':\n","                r2text = ' ' + dep_df.iloc[opinion_word+1]['opwo']\n","        except:\n","            pass\n","        return l2text + l1text + dep_df.iloc[opinion_word]['opwo'] + r1text + r2text "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9LDgNu7UY__"},"outputs":[],"source":["exp(-gamma * ||x-y||^2)"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1669923953026,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"ELwnDdE17OM2"},"outputs":[],"source":["owm = OpinionWordMiner(tokenizer)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":619},"executionInfo":{"elapsed":756,"status":"ok","timestamp":1669925734296,"user":{"displayName":"Kevin Scaria","userId":"09596160905637401311"},"user_tz":420},"id":"j3UzWTvi7OM3","outputId":"9b99387e-c441-4a40-9517-3fc74cd8cfce","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DP has probably the best service but their chicken is yummy. service\n"]},{"data":{"text/html":["\n","  <div id=\"df-43402ca7-5263-4c2a-90b8-93d37665cb43\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>opwo</th>\n","      <th>dep</th>\n","      <th>pos</th>\n","      <th>deps</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[CLS]</td>\n","      <td>0.681804</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>dp</td>\n","      <td>0.758891</td>\n","      <td>PROPN</td>\n","      <td>nsubj</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>has</td>\n","      <td>0.727692</td>\n","      <td>VERB</td>\n","      <td>ROOT</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>probably</td>\n","      <td>0.676997</td>\n","      <td>ADV</td>\n","      <td>advmod</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>the</td>\n","      <td>0.707547</td>\n","      <td>DET</td>\n","      <td>det</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>best</td>\n","      <td>0.695795</td>\n","      <td>ADJ</td>\n","      <td>amod</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>service</td>\n","      <td>1.000000</td>\n","      <td>NOUN</td>\n","      <td>dobj</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>but</td>\n","      <td>0.695976</td>\n","      <td>CCONJ</td>\n","      <td>cc</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>their</td>\n","      <td>0.721810</td>\n","      <td>PRON</td>\n","      <td>poss</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>chicken</td>\n","      <td>0.697497</td>\n","      <td>NOUN</td>\n","      <td>nsubj</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>is</td>\n","      <td>0.711135</td>\n","      <td>AUX</td>\n","      <td>conj</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>yummy</td>\n","      <td>0.712270</td>\n","      <td>ADJ</td>\n","      <td>acomp</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>.</td>\n","      <td>0.657434</td>\n","      <td>PUNCT</td>\n","      <td>punct</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>[SEP]</td>\n","      <td>0.663554</td>\n","      <td>None</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43402ca7-5263-4c2a-90b8-93d37665cb43')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-43402ca7-5263-4c2a-90b8-93d37665cb43 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-43402ca7-5263-4c2a-90b8-93d37665cb43');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["        opwo       dep    pos    deps\n","0      [CLS]  0.681804   None    None\n","1         dp  0.758891  PROPN   nsubj\n","2        has  0.727692   VERB    ROOT\n","3   probably  0.676997    ADV  advmod\n","4        the  0.707547    DET     det\n","5       best  0.695795    ADJ    amod\n","6    service  1.000000   NOUN    dobj\n","7        but  0.695976  CCONJ      cc\n","8      their  0.721810   PRON    poss\n","9    chicken  0.697497   NOUN   nsubj\n","10        is  0.711135    AUX    conj\n","11     yummy  0.712270    ADJ   acomp\n","12         .  0.657434  PUNCT   punct\n","13     [SEP]  0.663554   None    None"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","  <div id=\"df-a649b829-da2d-473d-a3d6-751aca0a744a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>opwo</th>\n","      <th>dep</th>\n","      <th>pos</th>\n","      <th>deps</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6</th>\n","      <td>service</td>\n","      <td>1.000000</td>\n","      <td>NOUN</td>\n","      <td>dobj</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>best</td>\n","      <td>0.695795</td>\n","      <td>ADJ</td>\n","      <td>amod</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a649b829-da2d-473d-a3d6-751aca0a744a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a649b829-da2d-473d-a3d6-751aca0a744a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a649b829-da2d-473d-a3d6-751aca0a744a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      opwo       dep   pos  deps\n","6  service  1.000000  NOUN  dobj\n","5     best  0.695795   ADJ  amod"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'service'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["# Testing on text inputs\n","# text, asp_word = df.loc[8, 'text'], 'bagels'\n","# text, asp_word = \"The cab ride was amazing but the driver was pathetic.\", 'ride'\n","# text, asp_word = \"The cab ride was amazing but the driver was pathetic.\", 'driver'\n","# text, asp_word = \"DelhiPalace has probably the best service and their chicken is yummy.\", 'chicken'\n","text, asp_word = \"DP has probably the best service but their chicken is yummy.\", 'service'\n","# text, asp_word = \"I goto Olive Garden for their beautiful ambience and calming music.\", 'music'\n","# text, asp_word = \"I goto Olive Garden for their beautiful ambience and calming music.\", 'ambience'\n","# text, asp_word = \"Cafe Bistro had very bad waitress, but I got there just for their amazing coffee.\", 'waitress'\n","# text, asp_word = \"Cafe Bistro had very bad waitress, but I got there just for their amazing coffee.\", 'coffee'\n","# text, asp_word = \"The food joint had some issues with their payments, but ambience was topnotch.\", 'payments'\n","# text, asp_word = \"The food joint had some issues with their payments, but ambience was topnotch.\", 'ambience'\n","# text, asp_word = \"I went to watch a movie. The actor was not good.\", 'actor'\n","\n","# text = resolve_coreference(text)\n","print(text, asp_word)\n","\n","owm.mine_opinion_words(text, asp_word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFK4uUD8TRIf"},"outputs":[],"source":["        ride driver . . . \n","ride\n","driver\n",".\n",".\n","."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uejc5rDB7OM3"},"outputs":[],"source":["## RULES \n","# dobj:amod\n","# nsubj:acomp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nz9qr2qm7OM3"},"outputs":[],"source":["# # Testing on text inputs\n","# text = df.loc[14, 'text']\n","# # text = \"The cab ride was amazing but the driver was pathetic.\"\n","# # text = \"Delhi palace has probably the best service and their chicken is mindblow.\"\n","\n","# text = resolve_coreference(text)\n","# mark_text = \"[CLS] \" + text + \" [SEP]\"\n","# print('Text: ', mark_text)\n","\n","# # Get word embeddings from BERT domain specific model\n","# wts, _ = get_word_embeddings(mark_text)\n","\n","# # Algining word embeddings for subwords by averaging\n","# word_emb = align_subwords_embeddings(mark_text, tokenizer, wts)\n","\n","# idx2word_map = defaultdict(list)\n","# for token, subword_idx in zip(_, tokenizer(mark_text).word_ids()[1:-1]):\n","#     idx2word_map[subword_idx].append(token.replace('#', ''))\n","\n","# for key, val in idx2word_map.items():\n","#     idx2word_map[key] = ''.join(val)\n","\n","# word2idx_map = {val:key for key, val in idx2word_map.items()}  \n","    \n","# we_tokens_pos_tags, we_tokens_deps = [], []\n","# spacy_tokens_pos_tags = {token.text.lower():token.pos_ for token in nlp(text)}\n","# spacy_tokens_deps = {token.text.lower():token.dep_ for token in nlp(text)}\n","\n","# for we_tokens in list(idx2word_map.values()):\n","#     if we_tokens in spacy_tokens_pos_tags:\n","#         we_tokens_pos_tags.append(spacy_tokens_pos_tags[we_tokens])\n","#         we_tokens_deps.append(spacy_tokens_deps[we_tokens])\n","#     else:\n","#         we_tokens_pos_tags.append(None)\n","#         we_tokens_deps.append(None)\n","\n","# print(word2idx_map)\n","# word_i = 8\n","# dep_df = pd.DataFrame(rbf_kernel(word_emb, word_emb, gamma)[word_i], columns = ['dep'], \n","#                       index = idx2word_map.values())\n","# dep_df['pos'] = we_tokens_pos_tags\n","# dep_df['deps'] = we_tokens_deps\n","# dep_df[dep_df['pos'] == 'ADJ'].sort_values(by = 'dep', ascending = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1DyEWOX7OM3"},"outputs":[],"source":["labels = {'kev', 'ben', 'sc'}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqMb1Dvf7OM3","scrolled":true},"outputs":[],"source":["sorted(att_scores.items(), key = lambda x: x, reverse = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4keUXyaW7OM3"},"outputs":[],"source":["token_we, tok_text = get_word_embeddings(mark_text)"]},{"cell_type":"markdown","metadata":{"id":"E_JkdtJT7OM3"},"source":["## Rule Based Approach"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYUeGdXC7OM4","scrolled":true},"outputs":[],"source":["ii = 9\n","tt = df.loc[ii, 'text']\n","tt = 'The bagels have an oustanding taste'\n","asp = df.loc[ii, 'terms']\n","print(tt, asp)\n","doc = nlp(tt)\n","print([to_nltk_tree(sent.root).pretty_print() for sent in doc.sents])\n","\n","dep_parser = []\n","for token in doc:\n","    dependency, governer_word, governer_pos, dependent_word, dependent_pos = token.dep_, token.head, token.head.pos_, token, token.pos_\n","    dep_parser.append([dependency, governer_word, governer_pos, dependent_word, dependent_pos])\n","    \n","# displacy.serve(doc, style='dep')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptzWYiNi7OM4","scrolled":true},"outputs":[],"source":["pd.DataFrame(dep_parser)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aW0tJ0_-7OM4"},"outputs":[],"source":["def to_nltk_tree(node):\n","    if node.n_lefts + node.n_rights > 0:\n","        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n","    else:\n","        return node.orth_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59s5b1wR7OM4","scrolled":false},"outputs":[],"source":["tr = to_nltk_tree(list(doc.sents)[0].root)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dP4yxLIS7OM4"},"outputs":[],"source":["def traverse_tree(tree):\n","    for subtree in tree:\n","        if type(subtree) == nltk.tree.Tree:\n","            traverse_tree(subtree)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6BZwatX7OM4"},"outputs":[],"source":["traverse_tree(tr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2YdyPv67OM4"},"outputs":[],"source":["# ii = 8\n","# tt = df.loc[ii, 'text']\n","# asp = df.loc[ii, 'terms']\n","# asp_term_sel = 'Bagels'\n","# print(tt, asp)\n","# doc = nlp(tt)\n","# print([to_nltk_tree(sent.root).pretty_print() for sent in doc.sents])\n","\n","\n","# predicates = {}\n","# for token in doc:\n","#     dependency, governer_word, governer_pos, dependent_word, dependent_pos = token.dep_, token.head, token.head.pos_, token, token.pos_\n","\n","# #     print(dependency, governer_word, governer_pos, dependent_word, dependent_pos)\n","    \n","#     if dependent_pos == 'ADJ':\n","#         predicates[dependent_word] = [dependent_word]\n","    \n","#     if dependency == 'nsubj':\n","#         sub = dependent_word\n","#         conn = \n","#         print('SUB: ', sub)\n","#     if dependency == 'dobj':\n","#         print(dependency, governer_word, governer_pos, dependent_word, dependent_pos)\n","#         obj = dependent_word\n","#         print('OBJ: ', obj)\n","        \n","#     if obj in predicates:\n","#         predicate = predicates[dependent_word]\n","#         print('PRED: ', predicate)\n","    \n","# displacy.serve(doc, style='dep')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"chT3tRjh7OM4"},"outputs":[],"source":["# ii = 9\n","# tt = df.loc[ii, 'text']\n","# asp = df.loc[ii, 'terms']\n","# print(tt, asp)\n","# doc = nlp(tt)\n","# displacy.serve(doc, style='dep')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJ23Sql27OM4"},"outputs":[],"source":["# # Run dependency parser and extract opinion words that are more than given threshold\n","# doc = stanzanlp('the bagels were outstanding as they were gummy and chewy.')\n","\n","# sentiment_score = defaultdict(int)\n","# for sent in doc.sentences:\n","#     for word in sent.words:\n","#         if word.deprel == \"flat:foreign\":\n","#             print(word.text, sent.words[word.head-1].text, TextBlob(word.text).sentiment.polarity)\n","#             if TextBlob(word.text).sentiment.polarity>=0.5 or TextBlob(word.text).sentiment.polarity<=-0.5:\n","#                 sentiment_score[sent.words[word.head-1].text] = TextBlob(word.text).sentiment.polarity"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"8a8422ae0b4d379eab6b04a6108e1cb8da685f55fb4b4e24ef69a5d2fb57f3c9"}}},"nbformat":4,"nbformat_minor":0}
