{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hgupta35/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk, os, math\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASTREX Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_strings(df, col):\n",
    "    \"\"\"\n",
    "    Reconstruct strings to dictionaries when loading csv/xlsx files.\n",
    "    \"\"\"\n",
    "    reconstructed_col = []\n",
    "    for text in df[col]:\n",
    "        if text != '[]' and isinstance(text, str):\n",
    "            text = text.replace('[', '').replace(']', '').replace('{', '').replace('}', '').split(\", '\")\n",
    "            req_list = []\n",
    "            for idx, pair in enumerate(text):\n",
    "                splitter = ': ' if ': ' in pair else ':'\n",
    "                if idx%2==0:\n",
    "                    reconstructed_dict = {} \n",
    "                    reconstructed_dict[pair.split(splitter)[0].replace(\"'\", '')] = pair.split(splitter)[1].replace(\"'\", '')\n",
    "                else:\n",
    "                    reconstructed_dict[pair.split(splitter)[0].replace(\"'\", '')] = pair.split(splitter)[1].replace(\"'\", '')\n",
    "                    req_list.append(reconstructed_dict)\n",
    "        else:\n",
    "            req_list = text\n",
    "        reconstructed_col.append(req_list)\n",
    "    df[col] = reconstructed_col\n",
    "    return df\n",
    "\n",
    "def extract_rowwise_aspect_polarity(df, on, key, min_val = None):\n",
    "    \"\"\"\n",
    "    Create duplicate records based on number of aspect term labels in the dataset.\n",
    "    Extract each aspect term for each row for reviews with muliple aspect term entries. \n",
    "    Do same for polarities and create new column for the same.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.iloc[0][on][0][key]\n",
    "    except:\n",
    "        df = reconstruct_strings(df, on)\n",
    "\n",
    "    df['len'] = df[on].apply(lambda x: len(x))\n",
    "    if min_val is not None:\n",
    "        df.loc[df['len'] == 0, 'len'] = min_val\n",
    "    df = df.loc[df.index.repeat(df['len'])]\n",
    "    df['record_idx'] = df.groupby(df.index).cumcount()\n",
    "    df['aspect'] = df[[on, 'record_idx']].apply(lambda x : (x[0][x[1]][key], x[0][x[1]]['polarity']) if len(x[0]) != 0 else ('',''), axis=1)\n",
    "    df['polarity'] = df['aspect'].apply(lambda x: x[-1])\n",
    "    df['aspect'] = df['aspect'].apply(lambda x: x[0])\n",
    "    df = df.drop(['len', 'record_idx'], axis=1).reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "def create_data_in_atsc_format(df, on, key, text_col, aspect_col, bos_instruction = '', \n",
    "                    delim_instruction = '', eos_instruction = ''):\n",
    "    \"\"\"\n",
    "    Prepare the data in the input format required.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "    df = extract_rowwise_aspect_polarity(df, on=on, key=key, min_val=1)\n",
    "    df['text'] = df[[text_col, aspect_col]].apply(lambda x: bos_instruction + x[0] + delim_instruction + x[1] + eos_instruction, axis=1)\n",
    "    df = df.rename(columns = {'polarity': 'labels'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Massive Pre Finetuning for Domain Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLM:\n",
    "    def __init__(self, text_input, text_column = 'text', seed = 42, test_size = None, chunk_size=128, model_ckpt='bert-large-uncased'):\n",
    "        self.seed = 42\n",
    "        self.test_size = test_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.text_column = text_column\n",
    "\n",
    "        # Load the BERT-large tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_ckpt)\n",
    "\n",
    "        if isinstance(text_input, list):\n",
    "            df = pd.DataFrame({text_column:text_input})\n",
    "            if test_size is not None:\n",
    "                train, test = train_test_split(df, random_state = self.seed, test_size = self.test_size)\n",
    "                self.dataset = DatasetDict({'train':Dataset.from_pandas(train), 'test':Dataset.from_pandas(test)})\n",
    "            else:\n",
    "                self.dataset = DatasetDict({'train':Dataset.from_pandas(df), 'test':Dataset.from_pandas(df)})\n",
    "        else:\n",
    "            self.dataset = text_input\n",
    "\n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        result = self.tokenizer(examples[self.text_column])\n",
    "        if self.tokenizer.is_fast:\n",
    "            result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "        return result\n",
    "\n",
    "\n",
    "    def group_texts(self, examples):\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        total_length = (total_length // self.chunk_size) * self.chunk_size\n",
    "        result = {\n",
    "            k: [t[i : i + self.chunk_size] for i in range(0, total_length, self.chunk_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "\n",
    "    def pre_finetune(self, root_path, mlm_proba = 0.15, batch_size=16, epochs = 8, return_trainer = False):\n",
    "        remove_cols = self.dataset['train'].column_names\n",
    "        tokenized_datasets = self.dataset.map(self.tokenize_function, batched=True, remove_columns=remove_cols)\n",
    "        lm_datasets = tokenized_datasets.map(self.group_texts, batched=True)\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm_probability=mlm_proba)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=root_path,\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=epochs,\n",
    "            save_strategy='epoch',\n",
    "            evaluation_strategy='epoch',\n",
    "            learning_rate=2e-5,\n",
    "            weight_decay=0.01,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            push_to_hub=False,\n",
    "            fp16=True,\n",
    "            logging_strategy='epoch',\n",
    "            save_total_limit = 2,\n",
    "            load_best_model_at_end=True \n",
    "        )\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=lm_datasets[\"train\"],\n",
    "            eval_dataset=lm_datasets[\"test\"],\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "\n",
    "        eval_results = self.trainer.evaluate()\n",
    "        print(f\">>> Initial Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "        print('Model training started ...')\n",
    "        self.trainer.train()\n",
    "        eval_results = self.trainer.evaluate()\n",
    "        print(f\">>> Final Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "        if return_trainer:\n",
    "            return self.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 800/800 [00:00<00:00, 11065.96 examples/s]\n",
      "Map: 100%|██████████| 800/800 [00:00<00:00, 14852.35 examples/s]\n",
      "Map: 100%|██████████| 800/800 [00:00<00:00, 9099.04 examples/s]\n",
      "Map: 100%|██████████| 800/800 [00:00<00:00, 9017.63 examples/s]\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initial Perplexity: 436334.35\n",
      "Model training started ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 07:54, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.480300</td>\n",
       "      <td>11.334242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.001900</td>\n",
       "      <td>10.116465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.319400</td>\n",
       "      <td>9.759091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9.616700</td>\n",
       "      <td>9.278866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>9.083100</td>\n",
       "      <td>8.533404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.349100</td>\n",
       "      <td>8.005946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.914400</td>\n",
       "      <td>7.939423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.823900</td>\n",
       "      <td>7.717659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.644700</td>\n",
       "      <td>7.426126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.433800</td>\n",
       "      <td>7.274204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>7.394300</td>\n",
       "      <td>7.173196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.126500</td>\n",
       "      <td>7.283614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>7.199700</td>\n",
       "      <td>7.195441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>7.082200</td>\n",
       "      <td>7.128549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7.132700</td>\n",
       "      <td>7.199237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.231300</td>\n",
       "      <td>7.090498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7.164200</td>\n",
       "      <td>6.957475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>7.117300</td>\n",
       "      <td>6.936259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>6.941100</td>\n",
       "      <td>6.938612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.888400</td>\n",
       "      <td>6.879988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Final Perplexity: 1139.78\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/data/data/hgupta35/causal_lm_kjs/unsup/Restaurants_Test.csv\")\n",
    "df = reconstruct_strings(df, 'aspectTerms')\n",
    "\n",
    "input_text = df['raw_text'].tolist()\n",
    "model_ckpt = \"microsoft/deberta-v2-xlarge\"\n",
    "root_path = f\"/data/data/hgupta35/causal_lm_kjs/unsup/{model_ckpt}\"\n",
    "chunk_size = 128\n",
    "\n",
    "ft = MLM(text_input=input_text, text_column='raw_text', chunk_size=chunk_size, model_ckpt=model_ckpt)\n",
    "ft.pre_finetune(root_path, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Aspect Terms Extraction using POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Did', 'Windows', 'functions']\n"
     ]
    }
   ],
   "source": [
    "# Define the input sentence\n",
    "sentence = \"The cab ride was amazing but the driver was pathetic.\"\n",
    "sentence = \"Did not enjoy the new Windows 8 and touchscreen functions.\"\n",
    "\n",
    "# Tokenize the sentence and perform POS tagging\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Extract the aspect terms (nouns) from the POS tags\n",
    "aspect_terms = [word for word, tag in pos_tags if tag.startswith('NN')]\n",
    "\n",
    "# Print the aspect terms\n",
    "print(aspect_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Opinion Mining using Dependency Reweighting using Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_aligned_subwords_embeddings_old(self, text, tokenized_text):\n",
    "#         \"\"\"\n",
    "#         Return word embeddings from hidden states when word piece/byte pair tokenizer is used.\n",
    "#         This method aligns the subwords into words by averaging the embeddings of subwords together\n",
    "#         \"\"\"\n",
    "#         tokens = self.tokenizer.tokenize(text)\n",
    "#         token_ids = tokenized_text.input_ids[0].detach().cpu().numpy()\n",
    "#         new_tokens = []\n",
    "#         new_token_ids = []\n",
    "#         new_position_ids = []\n",
    "\n",
    "#         for i in range(len(tokens)):\n",
    "#             if tokens[i].startswith('##') and i > 0:\n",
    "#                 new_tokens[-1] += tokens[i][2:]\n",
    "#                 if isinstance(new_token_ids[-1], list):\n",
    "#                     new_token_ids[-1] = new_token_ids[-1] + [token_ids[i]]\n",
    "#                     new_position_ids[-1] = new_position_ids[-1] + [i]\n",
    "#                 else:\n",
    "#                     new_token_ids[-1] = [new_token_ids[-1]] + [token_ids[i]]\n",
    "#                     new_position_ids[-1] = [new_position_ids[-1]] + [i]\n",
    "#             else:\n",
    "#                 new_tokens.append(tokens[i])\n",
    "#                 new_token_ids.append(token_ids[i])\n",
    "#                 new_position_ids.append(i)\n",
    "\n",
    "#         print(\"TOKENS ORIGINAL: \", tokens)\n",
    "#         print(\"TOKENS NEW: \", new_tokens)\n",
    "#         print(\"TOKENS IDS NEW: \", new_token_ids)\n",
    "#         print(\"TOKENS POS IDS NEW: \", new_position_ids)\n",
    "\n",
    "#         word_embeddings_aligned_list = []\n",
    "#         subword_embedding_list = []\n",
    "\n",
    "#         for pos_ids in new_position_ids:       \n",
    "#             if isinstance(pos_ids, list):\n",
    "#                 for sub_word_pos_id in pos_ids:\n",
    "#                     subword_embedding_list.append(self.token_level_embeddings[sub_word_pos_id])\n",
    "                \n",
    "#                 subword_embedding_list = torch.tensor(subword_embedding_list)\n",
    "#                 subword_embedding_list = subword_embedding_list.mean(dim=0)\n",
    "#                 word_embeddings_aligned_list.append(subword_embedding_list)\n",
    "#                 subword_embedding_list = []\n",
    "#             else:\n",
    "#                 word_embeddings_aligned_list.append(self.token_level_embeddings[pos_ids])\n",
    "\n",
    "#         word_embeddings_aligned = torch.tensor(word_embeddings_aligned_list)\n",
    "\n",
    "#         if self.score_by == 'attentions':\n",
    "#             # Block to handle the columns of the attention weights\n",
    "#             word_embeddings_aligned_list = []\n",
    "#             subword_embedding_list = []\n",
    "\n",
    "#             for pos_ids in new_position_ids:       \n",
    "#                 if isinstance(pos_ids, list):\n",
    "#                     for sub_word_pos_id in pos_ids:\n",
    "#                         subword_embedding_list.append(word_embeddings_aligned[:, sub_word_pos_id])\n",
    "                    \n",
    "#                     subword_embedding_list = [list(t) for t in zip(*subword_embedding_list)]\n",
    "#                     subword_embedding_list = torch.tensor(subword_embedding_list)\n",
    "#                     subword_embedding_list = subword_embedding_list.mean(dim=1).view(-1, 1)\n",
    "#                     word_embeddings_aligned_list.append(subword_embedding_list)\n",
    "#                     subword_embedding_list = []\n",
    "#                 else:\n",
    "#                     word_embeddings_aligned_list.append(word_embeddings_aligned[:, sub_word_pos_id].view(-1, 1))\n",
    "            \n",
    "#             word_embeddings_aligned = torch.cat(word_embeddings_aligned_list, dim=1)\n",
    "        \n",
    "#         return word_embeddings_aligned, new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpinionWordMiner:\n",
    "    \"\"\"\n",
    "    The class is written for a batch_size 1 scenario\n",
    "    TODO: Convert to a batch processing scenario \n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, model, gamma = 0.0005, \n",
    "                 we_layer_list=[-1], score_by='attention'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.token_level_embeddings = None\n",
    "        self.score_by = 'attentions' if score_by == 'attention' else 'hidden_states'\n",
    "\n",
    "\n",
    "    def _get_word_embeddings(self, tokenized_text):\n",
    "        \"\"\"\n",
    "        This function extracts the word/token embeddings from the specified hidden layers.\n",
    "        \"\"\"\n",
    "        tokenized_sentence_output = self.model(**tokenized_text)\n",
    "        tokenized_sentence_hidden_states = torch.stack(tokenized_sentence_output[self.score_by], dim=0)\n",
    "        tokenized_sentence_hidden_states = torch.squeeze(tokenized_sentence_hidden_states, dim=1)\n",
    "        if self.score_by == 'attentions':\n",
    "            # Extract the lower layer attention heads\n",
    "            tokenized_sentence_embeddings = tokenized_sentence_hidden_states[2] # (Num Heads, Seq Length, Seq Length)\n",
    "        else:\n",
    "            tokenized_sentence_embeddings = tokenized_sentence_hidden_states.permute(1,0,2) # (Num Layers, Seq Length, Emb_Size)\n",
    "\n",
    "\n",
    "        if self.score_by == 'attentions':\n",
    "            # Extract specific specific heads for attention based scoring\n",
    "            self.token_level_embeddings = tokenized_sentence_embeddings.mean(dim=0).detach().numpy()\n",
    "        else:\n",
    "            # Extract specific layer outputs for embedding based scoring\n",
    "            token_vecs_cat = []\n",
    "            for token in tokenized_sentence_embeddings:\n",
    "                cat_vec = torch.cat((\n",
    "                    token[0],\n",
    "                    token[1],\n",
    "                    token[2],\n",
    "                    # token[-1]\n",
    "                ), dim=0)\n",
    "                token_vecs_cat.append(cat_vec.detach().numpy())\n",
    "            self.token_level_embeddings = np.array(token_vecs_cat)\n",
    "        return self.token_level_embeddings, tokenized_sentence_output\n",
    "    \n",
    "\n",
    "    def _get_aligned_subwords_embeddings(self, text):\n",
    "        \"\"\"\n",
    "        Return word embeddings from hidden states when word piece/byte pair tokenizer is used.\n",
    "        This method aligns the subwords into words by averaging the embeddings of subwords together\n",
    "        \"\"\"\n",
    "        word_embeddings_aligned_list = []\n",
    "        index_handler_for_cols = []\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        new_tokens = word_tokenize(text)\n",
    "\n",
    "        print(\"TOKENS ORIGINAL: \", tokens)\n",
    "        print(\"TOKENS NEW: \", new_tokens)\n",
    "        \n",
    "        for word in new_tokens:\n",
    "            tokenized_token = self.tokenizer.tokenize(word)\n",
    "            start_idx = tokens.index(tokenized_token[0])\n",
    "            end_idx = start_idx + len(tokenized_token)\n",
    "            word_embeddings = self.token_level_embeddings[start_idx:end_idx]\n",
    "            # if word == 'car':\n",
    "            #     print(\"ll: \", word_embeddings)\n",
    "            if word_embeddings.shape[0] > 1:\n",
    "                word_embeddings =np.mean(word_embeddings, axis=0).reshape(1, -1)\n",
    "                index_handler_for_cols.append([start_idx, end_idx])\n",
    "\n",
    "            word_embeddings_aligned_list.append(word_embeddings)\n",
    "\n",
    "        word_embeddings_aligned = np.array(word_embeddings_aligned_list).squeeze(axis=1)\n",
    "\n",
    "        if self.score_by == 'attentions':\n",
    "            diff_idx_ = 0\n",
    "            for start_idx_, end_idx_ in index_handler_for_cols:\n",
    "                start_idx_-=diff_idx_\n",
    "                end_idx_-=diff_idx_\n",
    "                mean_val = np.mean(word_embeddings_aligned[:, start_idx_:end_idx_], axis=1, keepdims=True)\n",
    "                word_embeddings_aligned[:, [start_idx_]] = mean_val\n",
    "                word_embeddings_aligned = np.delete(word_embeddings_aligned, np.s_[start_idx_+1:end_idx_], axis=1)\n",
    "                diff_idx_ = end_idx_ - start_idx_-1\n",
    "\n",
    "        assert len(new_tokens) == word_embeddings_aligned.shape[0]\n",
    "        \n",
    "        if self.score_by == 'attentions':\n",
    "            assert len(new_tokens) == word_embeddings_aligned.shape[1]\n",
    "\n",
    "        return torch.tensor(word_embeddings_aligned), new_tokens\n",
    "    \n",
    "\n",
    "    def _filter_candidates(self, dataframe, shift_index_filter, pos_filter):\n",
    "        \"\"\"\n",
    "        Method to create rules for extracting compound phrases.\n",
    "        Arguments:\n",
    "        dataframe: This argument is the dataframe with the POS and dependency information.\n",
    "        index_filters: This argument handles the sequential indexes to check for the pos tags.\n",
    "        pos_filters: This argument handles the pos tags at each index in the dependency dataframe.\n",
    "        \"\"\"\n",
    "        filter_ = [(idx_, pos_) for idx_, pos_ in zip(shift_index_filter, pos_filter)]\n",
    "        filter_condition = reduce(lambda x, y: x & y, [dataframe['pos'].shift(pos_) == val_ for pos_, val_ in filter_])\n",
    "        \n",
    "        compound_phrase_idx = []\n",
    "        comp_phrase_record = []\n",
    "        for idx in dataframe[filter_condition].index:\n",
    "            if (idx + 1) < len(dataframe):\n",
    "                comp_phrase_record = [\n",
    "                    ' '.join([dataframe.loc[idx-idx_val]['opinion_word'] for idx_val in shift_index_filter]),\n",
    "                    dataframe.loc[idx]['attention_score'],\n",
    "                    '-'.join([dataframe.loc[idx-idx_val]['pos'] for idx_val in shift_index_filter]),\n",
    "                    dataframe.loc[idx]['dep']\n",
    "                    ]\n",
    "                for idx_val in shift_index_filter:\n",
    "                    compound_phrase_idx.append(idx-idx_val)\n",
    "                dataframe.loc[len(dataframe)] = comp_phrase_record\n",
    "        dataframe.drop(index=compound_phrase_idx, inplace=True)\n",
    "        return dataframe\n",
    "\n",
    "    \n",
    "    def mine_opinion_words(self, text, aspect_word, display_df = False):\n",
    "        # Tokenize the text\n",
    "        tokenized_text = self.tokenizer.encode_plus(text, add_special_tokens=False, return_tensors='pt')\n",
    "\n",
    "        # Query the word embeddings/attention_weights for each token\n",
    "        word_embeddings, temp_ = self._get_word_embeddings(tokenized_text)\n",
    "\n",
    "        # Align the word embeddings if the tokenizer splits words into sub words\n",
    "        word_embeddings, aligned_tokens = self._get_aligned_subwords_embeddings(text)\n",
    "\n",
    "        # Extract POS tags and Dependency tags\n",
    "        spacy_tokens_pos_tags = [token.pos_ for token in nlp(text)]\n",
    "        spacy_tokens_deps = [token.dep_ for token in nlp(text)]\n",
    "\n",
    "        try:\n",
    "            # Set default attention score for the aspect word\n",
    "            aspect_word_score = [0 for i in range(len(aligned_tokens))]\n",
    "\n",
    "            if self.score_by != 'attentions':\n",
    "                # Only use RBF kernel similarity if it is embedding based method\n",
    "                self_attention_matrix = rbf_kernel(word_embeddings, word_embeddings, self.gamma)\n",
    "            else:\n",
    "                # RBF Kernel is not required since attention weights already show where the Query is attending to other Key vectors\n",
    "                self_attention_matrix = word_embeddings\n",
    "\n",
    "            if ' ' in aspect_word:\n",
    "                drop_records = []\n",
    "                aspect_words = aspect_word.split()\n",
    "                combined_aspect_word_embeddings = []\n",
    "\n",
    "                for aspect_word in aspect_words:\n",
    "                    aspect_word_idx = aligned_tokens.index(aspect_word)\n",
    "                    drop_records.append(aspect_word_idx)\n",
    "                    combined_aspect_word_embeddings.append(self_attention_matrix[aspect_word_idx])\n",
    "                aspect_word_score = np.mean(combined_aspect_word_embeddings, axis=0)\n",
    "            else:\n",
    "                aspect_word_idx = aligned_tokens.index(aspect_word)\n",
    "                drop_records = [aspect_word_idx]\n",
    "                aspect_word_score = self_attention_matrix[:, aspect_word_idx]\n",
    "\n",
    "            dep_df = pd.DataFrame(aspect_word_score, columns = ['attention_score'], index=aligned_tokens)\n",
    "            dep_df['pos'] = spacy_tokens_pos_tags\n",
    "            dep_df['dep'] = spacy_tokens_deps            \n",
    "            dep_df = dep_df.reset_index().rename(columns = {'index':'opinion_word'})\n",
    "            dep_df.drop(index=drop_records, inplace=True) # Remove aspect word scores (since it will be highest)\n",
    "\n",
    "            # Reset index before applying rule based filtering\n",
    "            dep_df.reset_index(drop=True, inplace=True)\n",
    "            display(dep_df)\n",
    "\n",
    "            \"\"\"\n",
    "            TODO: Add option to generalize the rules \n",
    "            \"\"\"            \n",
    "    \n",
    "            # RULE 1: COMPOUND PHRASE EXTRACTION -> Compound Noun (Adjective + Noun) [AMOD]\n",
    "            dep_df = self._filter_candidates(dataframe=dep_df, \n",
    "                                             shift_index_filter=[0, -1], \n",
    "                                             pos_filter=['ADJ', 'NOUN'])\n",
    "\n",
    "            # RULE 2: COMPOUND PHRASE EXTRACTION -> Compound Noun (Adjective + Noun + Noun) [AMOD~COMPOUND]\n",
    "            dep_df = self._filter_candidates(dataframe=dep_df, \n",
    "                                             shift_index_filter=[0, -1, -2], \n",
    "                                             pos_filter=['ADJ', 'NOUN', 'NOUN'])\n",
    "\n",
    "            # RULE 3: COMPOUND PHRASE EXTRACTION -> Adverbial Phrase (Adverb + Adjective)\n",
    "            dep_df = self._filter_candidates(dataframe=dep_df, \n",
    "                                             shift_index_filter=[0, -1], \n",
    "                                             pos_filter=['ADV', 'ADJ'])\n",
    "        \n",
    "            # RULE 4: COMPOUND PHRASE EXTRACTION -> Adverbial Phrase (AdP + NOUN)\n",
    "            dep_df = self._filter_candidates(dataframe=dep_df, \n",
    "                                             shift_index_filter=[0, -1], \n",
    "                                             pos_filter=['ADP', 'NOUN'])\n",
    "\n",
    "            if display_df:\n",
    "                display(dep_df)\n",
    "\n",
    "            dep_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            # Final filters\n",
    "            dep_df = dep_df[(dep_df['pos'] == 'ADJ') | \\\n",
    "                            (dep_df['pos'] == 'ADJ-NOUN') |\\\n",
    "                            (dep_df['pos'] == 'ADJ-NOUN-NOUN') |\\\n",
    "                            (dep_df['pos'] == 'ADV-ADJ') |\\\n",
    "                            (dep_df['pos'] == 'ADP-NOUN')\n",
    "                            ]\n",
    "\n",
    "            if display_df:\n",
    "                display(dep_df)\n",
    "            if dep_df.shape[0] == 0:\n",
    "                return ''\n",
    "\n",
    "            # Candidate Reweighting\n",
    "            opinion_word = dep_df.sort_values(by = 'attention_score', ascending = False).head(1)['opinion_word'].values[0]\n",
    "            return opinion_word, None\n",
    "        except:\n",
    "            return 'NoOptinionTerm', None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS ORIGINAL:  ['i', 'charge', 'it', 'at', 'night', 'and', 'skip', 'taking', 'the', 'cord', 'with', 'me', 'because', 'of', 'the', 'good', 'battery', '##life', '.']\n",
      "TOKENS NEW:  ['I', 'charge', 'it', 'at', 'night', 'and', 'skip', 'taking', 'the', 'cord', 'with', 'me', 'because', 'of', 'the', 'good', 'batterylife', '.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opinion_word</th>\n",
       "      <th>attention_score</th>\n",
       "      <th>pos</th>\n",
       "      <th>dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>0.015123</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>charge</td>\n",
       "      <td>0.013026</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>PRON</td>\n",
       "      <td>dobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>night</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>skip</td>\n",
       "      <td>0.009113</td>\n",
       "      <td>VERB</td>\n",
       "      <td>conj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>taking</td>\n",
       "      <td>0.014255</td>\n",
       "      <td>VERB</td>\n",
       "      <td>xcomp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the</td>\n",
       "      <td>0.017507</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cord</td>\n",
       "      <td>0.017336</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>with</td>\n",
       "      <td>0.006101</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>me</td>\n",
       "      <td>0.010881</td>\n",
       "      <td>PRON</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>because</td>\n",
       "      <td>0.008047</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>of</td>\n",
       "      <td>0.007338</td>\n",
       "      <td>ADP</td>\n",
       "      <td>pcomp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the</td>\n",
       "      <td>0.017507</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>good</td>\n",
       "      <td>0.014061</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>.</td>\n",
       "      <td>0.021690</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   opinion_word  attention_score    pos    dep\n",
       "0             I         0.015123   PRON  nsubj\n",
       "1        charge         0.013026   VERB   ROOT\n",
       "2            it         0.010823   PRON   dobj\n",
       "3            at         0.008657    ADP   prep\n",
       "4         night         0.013295   NOUN   pobj\n",
       "5           and         0.007944  CCONJ     cc\n",
       "6          skip         0.009113   VERB   conj\n",
       "7        taking         0.014255   VERB  xcomp\n",
       "8           the         0.017507    DET    det\n",
       "9          cord         0.017336   NOUN   dobj\n",
       "10         with         0.006101    ADP   prep\n",
       "11           me         0.010881   PRON   pobj\n",
       "12      because         0.008047  SCONJ   prep\n",
       "13           of         0.007338    ADP  pcomp\n",
       "14          the         0.017507    DET    det\n",
       "15         good         0.014061    ADJ   amod\n",
       "16            .         0.021690  PUNCT  punct"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opinion_word</th>\n",
       "      <th>attention_score</th>\n",
       "      <th>pos</th>\n",
       "      <th>dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>0.015123</td>\n",
       "      <td>PRON</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>charge</td>\n",
       "      <td>0.013026</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>PRON</td>\n",
       "      <td>dobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>skip</td>\n",
       "      <td>0.009113</td>\n",
       "      <td>VERB</td>\n",
       "      <td>conj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>taking</td>\n",
       "      <td>0.014255</td>\n",
       "      <td>VERB</td>\n",
       "      <td>xcomp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the</td>\n",
       "      <td>0.017507</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cord</td>\n",
       "      <td>0.017336</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>with</td>\n",
       "      <td>0.006101</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>me</td>\n",
       "      <td>0.010881</td>\n",
       "      <td>PRON</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>because</td>\n",
       "      <td>0.008047</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>of</td>\n",
       "      <td>0.007338</td>\n",
       "      <td>ADP</td>\n",
       "      <td>pcomp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>the</td>\n",
       "      <td>0.017507</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>good</td>\n",
       "      <td>0.014061</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>.</td>\n",
       "      <td>0.021690</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>at night</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>ADP-NOUN</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   opinion_word  attention_score       pos    dep\n",
       "0             I         0.015123      PRON  nsubj\n",
       "1        charge         0.013026      VERB   ROOT\n",
       "2            it         0.010823      PRON   dobj\n",
       "5           and         0.007944     CCONJ     cc\n",
       "6          skip         0.009113      VERB   conj\n",
       "7        taking         0.014255      VERB  xcomp\n",
       "8           the         0.017507       DET    det\n",
       "9          cord         0.017336      NOUN   dobj\n",
       "10         with         0.006101       ADP   prep\n",
       "11           me         0.010881      PRON   pobj\n",
       "12      because         0.008047     SCONJ   prep\n",
       "13           of         0.007338       ADP  pcomp\n",
       "14          the         0.017507       DET    det\n",
       "15         good         0.014061       ADJ   amod\n",
       "16            .         0.021690     PUNCT  punct\n",
       "17     at night         0.008657  ADP-NOUN   prep"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opinion_word</th>\n",
       "      <th>attention_score</th>\n",
       "      <th>pos</th>\n",
       "      <th>dep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>good</td>\n",
       "      <td>0.014061</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>at night</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>ADP-NOUN</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   opinion_word  attention_score       pos   dep\n",
       "13         good         0.014061       ADJ  amod\n",
       "15     at night         0.008657  ADP-NOUN  prep"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I charge it at night and skip taking the cord with me because of the good batterylife.\n",
      "batterylife  -->  good\n"
     ]
    }
   ],
   "source": [
    "owm = OpinionWordMiner(tokenizer, model, gamma=0.003, score_by='attention')\n",
    "\n",
    "text = \"The cabdriver was amazing, but the car was in pathetic condition\"\n",
    "aspect_term = \"car\"\n",
    "\n",
    "text = \"I had chocolate for lunch. It was really delicious. But the ambience was decent.\"\n",
    "aspect_term = \"chocolate\"\n",
    "# aspect_term = \"ambience\"\n",
    "\n",
    "text = \"I charge it at night and skip taking the cord with me because of the good batterylife.\"\n",
    "aspect_term = \"batterylife\"\n",
    "xy, xx = owm.mine_opinion_words(text, aspect_term, display_df=True)\n",
    "print(text)\n",
    "print(aspect_term, \" --> \", xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prepositional modifier'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"prep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'bert-large-uncased'\n",
    "local_model = '/data/data/hgupta35/nps/unsup/bert-large-uncased-local/checkpoint-16'\n",
    "\n",
    "# model_ckpt = 'microsoft/deberta-v2-xlarge'\n",
    "# local_model = '/data/data/hgupta35/causal_lm_kjs/unsup/deberta-v2-xlarge/checkpoint-40'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForMaskedLM.from_pretrained(local_model, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/data/data/hgupta35/causal_lm_kjs/unsup/Restaurants_Test.csv\")\n",
    "# df = create_data_in_atsc_format(df, 'aspectTerms', 'term', 'raw_text', 'aspect')\n",
    "\n",
    "# idx = 0\n",
    "# print(df.iloc[idx]['text'])\n",
    "# print(df.iloc[idx]['aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_word_embedding = []\n",
    "# for token, token_id in zip(tokenizer.tokenize(\"The techpota is great\"), tokenizer.encode_plus(\"The techpota is great\")['input_ids'][1:-1]):\n",
    "#     print(token, token_id)\n",
    "#     if \"#\" in token:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Polarity Matching using Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing on text inputs\n",
    "# text = df.loc[14, 'text']\n",
    "# # text = \"The cab ride was amazing but the driver was pathetic.\"\n",
    "# # text = \"Delhi palace has probably the best service and their chicken is mindblow.\"\n",
    "\n",
    "# text = resolve_coreference(text)\n",
    "# mark_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "# print('Text: ', mark_text)\n",
    "\n",
    "# # Get word embeddings from BERT domain specific model\n",
    "# wts, _ = get_word_embeddings(mark_text)\n",
    "\n",
    "# # Algining word embeddings for subwords by averaging\n",
    "# word_emb = align_subwords_embeddings(mark_text, tokenizer, wts)\n",
    "\n",
    "# idx2word_map = defaultdict(list)\n",
    "# for token, subword_idx in zip(_, tokenizer(mark_text).word_ids()[1:-1]):\n",
    "#     idx2word_map[subword_idx].append(token.replace('#', ''))\n",
    "\n",
    "# for key, val in idx2word_map.items():\n",
    "#     idx2word_map[key] = ''.join(val)\n",
    "\n",
    "# word2idx_map = {val:key for key, val in idx2word_map.items()}  \n",
    "    \n",
    "# we_tokens_pos_tags, we_tokens_deps = [], []\n",
    "# spacy_tokens_pos_tags = {token.text.lower():token.pos_ for token in nlp(text)}\n",
    "# spacy_tokens_deps = {token.text.lower():token.dep_ for token in nlp(text)}\n",
    "\n",
    "# for we_tokens in list(idx2word_map.values()):\n",
    "#     if we_tokens in spacy_tokens_pos_tags:\n",
    "#         we_tokens_pos_tags.append(spacy_tokens_pos_tags[we_tokens])\n",
    "#         we_tokens_deps.append(spacy_tokens_deps[we_tokens])\n",
    "#     else:\n",
    "#         we_tokens_pos_tags.append(None)\n",
    "#         we_tokens_deps.append(None)\n",
    "\n",
    "# print(word2idx_map)\n",
    "# word_i = 8\n",
    "# dep_df = pd.DataFrame(rbf_kernel(word_emb, word_emb, gamma)[word_i], columns = ['dep'], \n",
    "#                       index = idx2word_map.values())\n",
    "# dep_df['pos'] = we_tokens_pos_tags\n",
    "# dep_df['deps'] = we_tokens_deps\n",
    "# dep_df[dep_df['pos'] == 'ADJ'].sort_values(by = 'dep', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on text inputs\n",
    "# text, asp_word = df.loc[8, 'text'], 'bagels'\n",
    "# text, asp_word = \"The cab ride was amazing but the driver was pathetic.\", 'ride'\n",
    "# text, asp_word = \"The cab ride was amazing but the driver was pathetic.\", 'driver'\n",
    "# text, asp_word = \"DelhiPalace has probably the best service and their chicken is yummy.\", 'chicken'\n",
    "# text, asp_word = \"DP has probably the best service but their chicken is yummy.\", 'service'\n",
    "# text, asp_word = \"I goto Olive Garden for their beautiful ambience and calming music.\", 'music'\n",
    "# text, asp_word = \"I goto Olive Garden for their beautiful ambience and calming music.\", 'ambience'\n",
    "# text, asp_word = \"Cafe Bistro had very bad waitress, but I got there just for their amazing coffee.\", 'waitress'\n",
    "# text, asp_word = \"Cafe Bistro had very bad waitress, but I got there just for their amazing coffee.\", 'coffee'\n",
    "# text, asp_word = \"The food joint had some issues with their payments, but ambience was topnotch.\", 'payments'\n",
    "# text, asp_word = \"The food joint had some issues with their payments, but ambience was topnotch.\", 'ambience'\n",
    "# text, asp_word = \"I went to watch a movie. The actor was not good.\", 'actor'\n",
    "\n",
    "# text = resolve_coreference(text)\n",
    "# print(text, asp_word)\n",
    "\n",
    "# owm.mine_opinion_words(text, asp_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_reponse(payload, model, API_KEY):\n",
    "    API_URL = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "    headers_ = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    print(API_URL)\n",
    "    print(headers_)\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers_, json=payload)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return \"Error: \" + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english\n",
      "{'Authorization': 'Bearer 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'}\n",
      "[{'entity_group': 'PER', 'score': 0.9991335868835449, 'word': 'Sarah Jessica Parker', 'start': 11, 'end': 31}, {'entity_group': 'PER', 'score': 0.9979913234710693, 'word': 'Jessica', 'start': 52, 'end': 59}]\n"
     ]
    }
   ],
   "source": [
    "input_ = {\"inputs\": \"My name is Sarah Jessica Parker but you can call me Jessica\", \n",
    "          \"options\":{\"wait_for_model\":True,\n",
    "                    \"use_cache\":True}\n",
    "         }\n",
    "MODEL = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "\n",
    "response = get_api_reponse(payload=input_, model=MODEL, API_KEY=API_TOKEN)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datagen_hg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
