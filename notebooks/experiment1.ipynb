{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kscaria/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import stanfordnlp\n",
    "from textblob import TextBlob\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER, CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "infixes = (\n",
    "        LIST_ELLIPSES\n",
    "        + LIST_ICONS\n",
    "        + [\n",
    "            r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
    "            r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
    "                al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
    "            ),\n",
    "            r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
    "            r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "infix_re = compile_infix_regex(infixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfLearnABSA:\n",
    "    def __init__(self, spacy_model = '', coref_res_url = '', dep_parser_url = '', stanfordnlp_lang = 'en', coref_remove_word = None):\n",
    "        print('Loading models and setting up environment ...')\n",
    "        if spacy_model == '':\n",
    "            spacy_model = \"en_core_web_sm\"\n",
    "\n",
    "        if coref_res_url == '':\n",
    "            coref_res_url = \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\"\n",
    "        \n",
    "        if dep_parser_url == '':\n",
    "            dep_parser_url = \"https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz\"\n",
    "        \n",
    "        if not coref_remove_word:\n",
    "            self.coref_remove_word = []\n",
    "        else:\n",
    "            self.coref_remove_word = coref_remove_word\n",
    "\n",
    "        # self.nlp = spacy.load(spacy_model)\n",
    "        self.nlp_without_hyphen = spacy.load(spacy_model)\n",
    "        self.nlp_without_hyphen.tokenizer.infix_finditer = infix_re.finditer\n",
    "        self.coreference_resolver = Predictor.from_path(coref_res_url)\n",
    "        # self.dependency_parser = Predictor.from_path(dep_parser_url)\n",
    "        # self.stanford_dependency_parser = stanfordnlp.Pipeline(processors='tokenize,mwt,pos,lemma,depparse', lang=stanfordnlp_lang)\n",
    "        print('Models loaded and environment setup completed!!')\n",
    "\n",
    "    def resolve_coreference(self, text, remove_words = []):\n",
    "        coref_res_pred = self.coreference_resolver.predict(text)\n",
    "        coref_resolved_text = self.coreference_resolver.coref_resolved(text)\n",
    "        for cluster in coref_res_pred['clusters']:\n",
    "            coref_word, coref_dep = cluster[0], cluster[1]\n",
    "            coref_list = text.split()[coref_word[0]:coref_word[1]+1]\n",
    "            for bad_word in remove_words:\n",
    "                try:\n",
    "                    coref_list.remove(bad_word)\n",
    "                except:\n",
    "                    pass\n",
    "            coref_hyphenated_text = '-'.join(coref_list)\n",
    "            coref_hyphenated_text = re.sub('[^A-Za-z0-9-]+', ' ', coref_hyphenated_text).strip()\n",
    "            coref_spaced_text = ' '.join(coref_list)\n",
    "            coref_spaced_text = re.sub('[^A-Za-z0-9-]+', ' ', coref_spaced_text).strip()\n",
    "            coref_resolved_text = coref_resolved_text.replace(coref_spaced_text, coref_hyphenated_text)\n",
    "        return coref_resolved_text\n",
    "\n",
    "\n",
    "    def dependency_parser(self, resolved_text):\n",
    "        coref_res_doc = self.nlp_without_hyphen(resolved_text)\n",
    "        aspect_target_opinion = []\n",
    "        for token in coref_res_doc:\n",
    "\n",
    "            #Rule 1 - Using nsubj\n",
    "            dependency, governer_word, governer_pos, dependent_word, dependent_pos = token.dep_, token.head, token.head.pos_, token, token.pos_\n",
    "            if dependency == 'nsubj':\n",
    "                pass\n",
    "            \n",
    "            # Rule 2 - Using dobj:\n",
    "            if dependency == 'dobj':\n",
    "                if 'VERB' == governer_pos:\n",
    "                    aspect_target_opinion.append({'target':dependent_word.text, 'opinion':governer_word.text, 'polarity': TextBlob(governer_word.text).sentiment.polarity})\n",
    "\n",
    "            # Rule 3 - Extract all amod\n",
    "            if dependency == 'amod':\n",
    "                if dependent_pos == 'ADJ':\n",
    "                    aspect_target_opinion.append({'target':dependent_word.text, 'opinion':governer_word.text, 'polarity': TextBlob(dependent_word.text).sentiment.polarity})\n",
    "\n",
    "        return aspect_target_opinion\n",
    "\n",
    "\n",
    "    def get_parsed_output(self, input_, coref_remove_word = []):\n",
    "        if self.coref_remove_word == []:\n",
    "            self.coref_remove_word = coref_remove_word\n",
    "\n",
    "        if type(input_) == str:\n",
    "            resolved_text = self.resolve_coreference(input_, self.coref_remove_word)\n",
    "            return [(token.dep_, token.head, token.head.pos_, token, token.pos_) for token in self.nlp_without_hyphen(resolved_text)]\n",
    "        else:\n",
    "            raise Exception(\"parsed_outputs is only available for strings.\")\n",
    "\n",
    "\n",
    "    def fit(self, input_, coref_remove_word = []):\n",
    "        if self.coref_remove_word == []:\n",
    "            self.coref_remove_word = coref_remove_word\n",
    "\n",
    "        if type(input_) == str:\n",
    "            print('Resolving coreference ..')\n",
    "            coref_res_test = self.resolve_coreference(input_, self.coref_remove_word)\n",
    "            print('Resolving coreference completed ..')\n",
    "            print('Mining aspect target and opinions ...')\n",
    "            aspects = self.dependency_parser(coref_res_test)\n",
    "            print('Aspect target and opinions extracted!! ...') \n",
    "        return aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models and setting up environment ...\n",
      "Models loaded and environment setup completed!!\n"
     ]
    }
   ],
   "source": [
    "sl = SelfLearnABSA(coref_remove_word = ['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I enjoyed the screen resolution, it is amazing for such a cheap laptop.\"\n",
    "# text = \"But the staff was so horrible to us.\"\n",
    "# text = \"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\"\n",
    "# text = \"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\"\n",
    "# text = \"contracting covid after vaccines are proven but before receiving a shot would be like being a war casualty.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I enjoyed the-screen-resolution, the-screen-resolution is amazing for such a cheap laptop.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl.resolve_coreference(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving coreference ..\n",
      "Resolving coreference completed ..\n",
      "Mining aspect target and opinions ...\n",
      "Aspect target and opinions extracted!! ...\n",
      "\n",
      "[{'target': 'shot', 'opinion': 'receiving', 'polarity': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "asp_pairs = sl.fit(text)\n",
    "print()\n",
    "print(asp_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AllenNLP dependency parsing\n",
    "# dep_parser_pred = dependency_parser.predict(coref_resolved_text)\n",
    "\n",
    "# # Stanford dependency parsing\n",
    "# st_dep_parser_pred = stanford_dependency_parser(text)\n",
    "# st_dep_parser_pred.sentences[0].print_dependencies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
